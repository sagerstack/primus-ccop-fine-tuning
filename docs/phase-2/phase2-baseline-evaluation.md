# Phase 2: Baseline Evaluation Methodology and Process

## Overview

Phase 2 implements a comprehensive baseline evaluation framework to systematically assess the untuned Llama-Primus-Reasoning model's capability in CCoP 2.0 compliance reasoning tasks. This phase serves a diagnostic purpose rather than production validation, establishing performance baselines across 21 specialized benchmarks before domain-adapted fine-tuning [40]. The evaluation adopts a fine-tuning-centric research methodology where a low baseline score (≥15% threshold) is intentionally designed to justify the necessity of domain adaptation through parameter-efficient fine-tuning techniques. Unlike traditional model evaluation frameworks that prioritize deployment readiness, Phase 2 focuses on identifying systematic weaknesses in compliance reasoning, gap analysis, risk assessment, and audit-style interpretation that fine-tuning is specifically positioned to address [40].

The baseline screening infrastructure was refined from initial Phase 1 implementation to support benchmark-aware evaluation across multiple scoring tiers, incorporating semantic equivalence assessment for reasoning-based tasks, key-fact recall for completeness measurement, and grounding checks for hallucination detection [41]. This evaluation methodology explicitly separates compliance reasoning capabilities—which can be improved through fine-tuning—from regulatory retrieval and citation accuracy, which are better addressed through retrieval-augmented generation (RAG) mechanisms in future work. The architectural implementation follows clean architecture principles with domain-driven design, ensuring clear separation between presentation (CLI interface), application (use case orchestration), domain (scoring logic and business rules), and infrastructure (repository adapters and external service gateways).

## Benchmark Framework Design

The Phase 2 evaluation framework comprises 21 distinct benchmarks (B1-B21) organized into seven thematic categories addressing different dimensions of compliance reasoning capability [40]. The benchmarks were refined from initial clause citation-oriented metrics to focus specifically on audit-style compliance reasoning following extended analysis of related work on domain-specific language model evaluation. Benchmarks B1-B15 evaluate core compliance reasoning capabilities highly sensitive to domain-adapted fine-tuning, including applicability determination, compliance classification, conditional reasoning, scenario-to-control mapping, control requirement comprehension, control intent understanding, gap identification, gap prioritization, risk identification, risk justification coherence, risk severity assessment, audit perspective alignment, evidence expectation awareness, and remediation recommendation quality [40].

Benchmarks B16-B19 assess governance understanding and reasoning stability across scenarios, incorporating Singapore-specific regulatory context such as CIIO (Critical Information Infrastructure Owner) accountability frameworks, CSA (Cyber Security Agency) oversight requirements, and policy-versus-practice distinctions critical to CCoP 2.0 audit interpretation. Benchmarks B20-B21 serve as lightweight grounding and safety checks designed to detect over-specification (introduction of unsupported requirements beyond CCoP text) and regulatory hallucination (fabrication of non-existent CCoP obligations) without relying on retrieval-based citation mechanisms [40]. The benchmark framework explicitly includes foundational regulatory comprehension to ensure that subsequent compliance reasoning is applied only within the correct regulatory scope, with CCoP applicability and core terminology evaluated upfront before higher-order gap analysis and risk reasoning.

## Three-Tier Evaluation Methodology

Phase 2 implements a stratified three-tier evaluation methodology balancing objectivity, expert judgment, and scalability across the 21-benchmark framework [41]. Tier 1 employs binary metrics for objective pass/fail determination on Benchmarks B1 (CCoP Applicability & Core Terminology), B2 (Compliance Classification Accuracy), and B21 (Regulatory Hallucination Rate), utilizing automated scoring based on label matching and rule-based hallucination pattern detection with no inter-rater reliability requirements due to deterministic evaluation logic. Tier 2 applies expert rubric metrics for Benchmarks B7 (Gap Identification Quality), B10 (Risk Justification Coherence), B14 (Remediation Recommendation Quality), and B16 (Residual Risk Awareness), scoring responses on a 1-5 scale across four evaluation criteria—accuracy, completeness, practicality, and clarity—with inter-rater agreement threshold of ≥80% to ensure consistent expert assessment [41].

Tier 3 utilizes assisted judgment combining LLM-as-judge evaluation with human validation for Benchmarks B12 (Audit Perspective Alignment), B13 (Evidence Expectation Awareness), and B20 (Over-Specification Avoidance), implementing structured rubric evaluation executed by a general-purpose language model with human validation conducted on ≥20% of samples and human override authority when automated confidence falls below 0.8 threshold [41]. Benchmarks not explicitly assigned to Tiers 1-3 (B3-B11, B15, B17-B19) employ reasoning-focused evaluation combining semantic equivalence assessment for correct paraphrases and audit-style explanations with key-fact recall measurement to ensure completeness of compliance reasoning. This tiered approach reflects standard practice for evaluating reasoning-centric language model capabilities in specialized domains while maintaining methodological rigor appropriate for diagnostic baseline assessment.

## Scoring Methodology and Metric Aggregation

The Phase 2 scoring methodology implements benchmark-aware evaluation where accuracy interpretation varies based on benchmark type rather than applying uniform lexical similarity metrics across all test cases [41]. Classification benchmarks (applicability determination, compliance classification) utilize label matching with three-level scoring: exact match (1.0), partial match (0.7), and incorrect (0.0). Reasoning benchmarks (interpretation, gap analysis, risk reasoning, remediation) assess semantic equivalence allowing correct paraphrases and audit-style explanations to receive appropriate scoring rather than penalizing variation in surface-level wording. Safety benchmarks evaluate hallucination resistance and over-specification through binary pass/fail checks based on forbidden claim detection and regulatory fabrication pattern matching [41].

Completeness measurement employs key-fact recall methodology where each test case defines 3-8 atomic regulatory facts that must be covered in model responses, with completeness score calculated as the ratio of covered facts to total required facts [41]. Key facts represent essential compliance requirements or reasoning elements extracted from ground truth annotations, with coverage determined through key-term matching where 60% of significant terms (words exceeding 4 characters) from each fact must appear in the model response to count as covered. Regulatory grounding assessment identifies hallucination through pattern matching for fabricated CCoP obligations, incorrect legal definitions, and misleading clause references, while explicitly not penalizing appropriate uncertainty or conditional reasoning language such as "depends on the implemented controls" which reflects legitimate compliance reasoning behavior [41].

The overall score for each test case is computed through weighted metric aggregation: Overall Score = Σ(metric.value × metric.weight) / Σ(metric.weight), maintaining continuity with mid-term report evaluation formulation [41]. Standard metric weights are: accuracy (1.0), completeness (0.8), citation accuracy when applicable (1.0), hallucination resistance (1.0, calculated as 1 minus hallucination rate to preserve higher-is-better convention), terminology accuracy (0.9), classification accuracy (1.0), violation detection (1.0), and grounding (1.0). An optional concision metric may be applied to discourage excessively verbose responses that dilute correctness, used as a secondary signal without dominating overall scoring. Pass/fail determination for each test case follows phase-aware threshold logic: baseline phase ≥15%, fine-tuned phase ≥50%, deployment phase ≥85%, with explicit threshold override capability via command-line parameter when evaluation context requires deviation from phase defaults [41].

## Test Case Structure and Ground Truth Format

Test cases are stored in JSONL (JSON Lines) format with auto-discovery mechanism scanning for files matching pattern `b*.jsonl` in the designated test cases directory, with each file containing test cases for a specific benchmark type [40][41]. The test case entity structure comprises core identification fields (test_id following Bxx-nnn format, benchmark_type from B1-B21, section reference to CCoP 2.0 structure, clause_reference when applicable), difficulty classification (Low, Medium, High, Critical), the compliance reasoning question (minimum 50 characters), expected response providing ground truth answer, evaluation criteria dictionary specifying scoring strategy and benchmark-specific guidance, and metadata including domain classification (IT, OT, or IT/OT) for infrastructure applicability tracking.

Phase 2 introduces enhanced fields specifically supporting fine-tuning-centric evaluation: key_facts array containing 3-8 atomic regulatory statements required for completeness assessment, expected_label for classification benchmarks enabling label-based accuracy scoring, and forbidden_claims array listing fabricated or unsupported regulatory assertions that trigger hallucination penalties [41]. Test case validation enforces business rules including test_id format compliance (regex pattern `^B\d{1,2}-\d{3}$`), test_id prefix matching benchmark_type to ensure organizational consistency, question minimum length requirement, mandatory expected_response and evaluation_criteria fields, and test_id uniqueness across the entire test case repository. The repository implementation provides flexible loading operations supporting batch loading by benchmark type, specific test ID retrieval, and complete dataset enumeration with caching for performance optimization during iterative evaluation runs.

## Evaluation Pipeline Architecture and Flow

The evaluation pipeline implements a layered architecture following domain-driven design principles with clear separation of concerns across five distinct layers [41]. The presentation layer exposes CLI evaluation commands accepting parameters for model name, benchmark selection, test ID filters, inference temperature, result persistence flag, and evaluation phase designation. The application layer orchestrates evaluation through the EvaluateModelUseCase which sequences test case loading (by benchmark or specific IDs), model availability verification, iterative test case evaluation combining model inference with benchmark-specific scoring, optional result persistence to JSON repository, and summary generation aggregating performance by benchmark category and difficulty level.

The domain layer encapsulates core business logic through entities (TestCase representing ground truth, ModelResponse containing LLM output with analysis methods, EvaluationResult combining test case, response, and computed metrics), value objects (BenchmarkType supporting flexible B1-B99 identification with description parsing, EvaluationMetric as immutable weighted score, DifficultyLevel and CCoPSection for classification), and domain services (ScoringService implementing benchmark-aware scoring router dispatching to 21 specialized scoring functions across the three-tier evaluation methodology). The infrastructure layer provides adapter implementations for external systems including JSONLTestCaseRepository with auto-discovery and caching, JSONResultRepository persisting evaluation outcomes with rich metadata, OllamaGateway handling LLM communication through REST API with retry logic and error handling, and structured logging adapters supporting both development and production observability requirements.

The evaluation flow executes as follows: test case repository auto-discovers JSONL files and parses ground truth with validation; CLI invokes EvaluateModelUseCase with evaluation parameters; use case loads test cases filtered by benchmark or ID list; model gateway verifies Llama-Primus-Reasoning availability through Ollama server; for each test case, the gateway generates model response capturing content, token count, and latency; benchmark router identifies test case benchmark type and dispatches to appropriate scoring function (Tier 1 binary, Tier 2 rubric, Tier 3 LLM-judge, or reasoning semantic+key-fact); scoring function computes individual metrics (accuracy, completeness, grounding) with benchmark-specific interpretation; metrics undergo weighted aggregation producing overall score; evaluation result entity finalizes with pass/fail determination against phase threshold; results optionally persist to JSON repository; evaluation summary aggregates statistics by benchmark and difficulty with pass rate calculation. This architecture enables extensible benchmark addition, flexible scoring strategy modification, and independent infrastructure component replacement while maintaining domain logic integrity.

## Result Interpretation and Diagnostic Analysis

Phase 2 results are interpreted diagnostically rather than for production readiness assessment, with low baseline scores serving as evidence supporting the fine-tuning research hypothesis [41]. A low overall score combined with high completeness indicates verbosity or weak precision where the model provides comprehensive coverage of required facts but introduces excessive irrelevant content or lacks concise compliance judgment. A low overall score with low completeness indicates fundamental knowledge or reasoning gaps where the model fails to identify essential regulatory requirements or omits critical compliance considerations. Detection of hallucination (fabricated CCoP obligations, incorrect regulatory definitions, misleading clause references) indicates unsafe behavior requiring immediate attention before any production consideration, as hallucination in compliance reasoning contexts poses direct regulatory risk and undermines trust in automated compliance assessment systems.

The diagnostic baseline evaluation identifies systematic weaknesses across seven thematic categories: applicability and scope determination (whether CCoP applies to specific systems, digital boundary identification, CII classification logic); compliance judgment and classification (determining compliant versus non-compliant configurations, conditional compliance with compensating controls); control relevance and interpretation (mapping scenarios to applicable CCoP control domains, understanding literal requirements versus underlying intent); gap analysis capability (identifying missing controls, prioritizing remediation based on risk, recognizing common compliance failure patterns); risk reasoning quality (identifying compliance-specific risks, providing coherent justifications, assessing severity proportionally); audit perspective alignment (reasoning from CSA auditor viewpoint, understanding evidence expectations, applying regulatory interpretation consistent with enforcement practice); remediation recommendation quality (proposing practical and feasible corrective actions, maintaining awareness of residual risks post-remediation). This comprehensive diagnostic assessment establishes the performance baseline against which fine-tuning improvements will be measured in subsequent project phases.

## References (Baseline Evaluation)

[40] "Evaluation Benchmarks for CCoP 2.0," Phase 2 Technical Documentation, studio-ssdlc project, Dec. 2025. [Internal]. Available: docs/phase-2/benchmarks-updated.md

[41] "Scoring Methodology – Phase 2 (Fine-Tuning Evaluation)," Phase 2 Technical Documentation, studio-ssdlc project, Dec. 2025. [Internal]. Available: docs/phase-2/scoring-methodology-updated.md

---

**Next Steps:**
- Execute baseline evaluation on Llama-Primus-Reasoning (quantized 4-bit model)
- Generate comprehensive diagnostic report identifying systematic weaknesses
- Prioritize benchmark categories for fine-tuning dataset creation
- Establish performance improvement targets for Phase 3 post-fine-tuning evaluation
