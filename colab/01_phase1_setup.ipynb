{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase1_setup"
   },
   "source": [
    "# Phase 1: Environment Setup and Model Loading\n",
    "\n",
    "## Overview\n",
    "This notebook sets up the Google Colab environment for the CCoP 2.0 fine-tuning project.\n",
    "\n",
    "### Objectives:\n",
    "1. ‚úÖ Clone project repository from GitHub\n",
    "2. ‚úÖ Install required dependencies\n",
    "3. ‚úÖ Set up GPU environment\n",
    "4. ‚úÖ Load and test Llama-Primus-Reasoning model\n",
    "5. ‚úÖ Configure API access for comparative models\n",
    "\n",
    "### Requirements:\n",
    "- GPU Runtime (T4 or A100 recommended)\n",
    "- Sufficient disk space (~50GB)\n",
    "- Internet access for model downloads\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpu_check"
   },
   "source": [
    "## 1. GPU Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specifications\n",
    "import torch\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(\"üîß GPU Environment Check\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# GPU Check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU available. Please enable GPU runtime.\")\n",
    "    print(\"Go to Runtime > Change runtime type > GPU\")\n",
    "\n",
    "# System Memory\n",
    "memory_gb = psutil.virtual_memory().total / 1024**3\n",
    "print(f\"‚úÖ System Memory: {memory_gb:.1f} GB\")\n",
    "\n",
    "# Disk Space\n",
    "disk_usage = psutil.disk_usage('/')\n",
    "free_gb = disk_usage.free / 1024**3\n",
    "print(f\"‚úÖ Available Disk Space: {free_gb:.1f} GB\")\n",
    "\n",
    "if free_gb < 50:\n",
    "    print(\"‚ö†Ô∏è  Warning: Less than 50GB available. Consider clearing Colab files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "github_clone"
   },
   "source": [
    "## 2. Clone Project Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the project repository\n",
    "import os\n",
    "\n",
    "# Repository configuration\n",
    "REPO_URL = \"https://github.com/sagerstack/primus-ccop-fine-tuning.git\"\n",
    "BRANCH = \"feature/phase1-setup\"  # Change to \"main\" for production\n",
    "PROJECT_DIR = \"/content/studio-ssdlc\"\n",
    "\n",
    "print(\"üì• Cloning Project Repository\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Remove existing directory if it exists\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    !rm -rf {PROJECT_DIR}\n",
    "\n",
    "# Clone the repository\n",
    "print(f\"Cloning {REPO_URL} (branch: {BRANCH})...\")\n",
    "!git clone -b {BRANCH} {REPO_URL} {PROJECT_DIR}\n",
    "\n",
    "# Change to project directory\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"‚úÖ Repository cloned to: {PROJECT_DIR}\")\n",
    "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify repository structure\n",
    "print(\"\\nüìÅ Repository Structure:\")\n",
    "!ls -la\n",
    "\n",
    "# Show key directories\n",
    "print(\"\\nüìÇ Key Directories:\")\n",
    "for dir_name in ['src', 'data', 'config', 'colab', 'benchmarks']:\n",
    "    if os.path.exists(dir_name):\n",
    "        print(f\"‚úÖ {dir_name}/\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_name}/ (missing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_dependencies"
   },
   "source": [
    "## 3. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "print(\"üì¶ Installing Dependencies\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Core ML libraries\n",
    "print(\"Installing PyTorch and CUDA libraries...\")\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Transformers and ML libraries\n",
    "print(\"Installing Transformers and ML libraries...\")\n",
    "!pip install transformers accelerate bitsandbytes datasets tokenizers\n",
    "\n",
    "# Fine-tuning libraries\n",
    "print(\"Installing fine-tuning libraries (QLoRA/PEFT)...\")\n",
    "!pip install peft trl\n",
    "\n",
    "# Google Cloud and Drive APIs\n",
    "print(\"Installing Google Cloud APIs...\")\n",
    "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "!pip install google-cloud-storage PyDrive2\n",
    "\n",
    "# API integrations\n",
    "print(\"Installing API client libraries...\")\n",
    "!pip install openai anthropic requests httpx\n",
    "\n",
    "# Data processing and utilities\n",
    "print(\"Installing data processing libraries...\")\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn tqdm rich\n",
    "\n",
    "# Configuration and environment\n",
    "print(\"Installing configuration libraries...\")\n",
    "!pip install pydantic pydantic-settings pyyaml python-dotenv\n",
    "\n",
    "print(\"‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "environment_setup"
   },
   "source": [
    "## 4. Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_env"
   },
   "outputs": [],
   "source": [
    "# Set up environment configuration\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚öôÔ∏è  Environment Configuration\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load configuration\n",
    "config_path = Path(\"config/environment/development.yaml\")\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "    print(f\"‚úÖ Environment: {config['app']['environment']}\")\n",
    "    print(f\"‚úÖ App Name: {config['app']['name']}\")\n",
    "else:\n",
    "    print(\"‚ùå Configuration file not found\")\n",
    "    config = {}\n",
    "\n",
    "# Create necessary directories\n",
    "dirs_to_create = [\n",
    "    \"data/benchmark\",\n",
    "    \"models/base\",\n",
    "    \"models/checkpoints\",\n",
    "    \"benchmarks/results\",\n",
    "    \"logs\"\n",
    "]\n",
    "\n",
    "for dir_path in dirs_to_create:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ Created directory: {dir_path}\")\n",
    "\n",
    "# Set up Python path\n",
    "import sys\n",
    "if \"/content/studio-ssdlc/src\" not in sys.path:\n",
    "    sys.path.insert(0, \"/content/studio-ssdlc/src\")\n",
    "    print(\"‚úÖ Added src/ to Python path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_setup"
   },
   "source": [
    "## 5. Model Setup and Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_auth"
   },
   "outputs": [],
   "source": [
    "# Set up model authentication\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "\n",
    "print(\"üîê Model Authentication Setup\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Hugging Face authentication\n",
    "print(\"\\nü§ó Hugging Face Authentication:\")\n",
    "print(\"Please enter your Hugging Face token when prompted.\")\n",
    "print(\"You can get a token from: https://huggingface.co/settings/tokens\")\n",
    "\n",
    "# Note: Uncomment the following line and run it to login\n",
    "# notebook_login()\n",
    "\n",
    "# For now, we'll check if token is already available\n",
    "if os.environ.get('HUGGINGFACE_HUB_TOKEN'):\n",
    "    print(\"‚úÖ Hugging Face token found in environment\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Please set HUGGINGFACE_HUB_TOKEN or run notebook_login()\")\n",
    "\n",
    "# Check API keys (these should be set as environment variables)\n",
    "print(\"\\nüîë API Keys Check:\")\n",
    "api_keys = {\n",
    "    'OpenAI': os.environ.get('OPENAI_API_KEY'),\n",
    "    'Anthropic': os.environ.get('ANTHROPIC_API_KEY'),\n",
    "    'DeepSeek': os.environ.get('DEEPSEEK_API_KEY')\n",
    "}\n",
    "\n",
    "for api, key in api_keys.items():\n",
    "    if key:\n",
    "        print(f\"‚úÖ {api} API key: {'*' * 10}{key[-4:] if len(key) > 4 else key}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {api} API key: Not set\")\n",
    "\n",
    "print(\"\\nüí° To set API keys in Colab:\")\n",
    "print(\"1. Click on the key icon in the left sidebar\")\n",
    "print(\"2. Add new secret for each API key\")\n",
    "print(\"3. Enable notebook access to the secrets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_loading"
   },
   "source": [
    "## 6. Load Llama-Primus-Reasoning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load Llama-Primus-Reasoning model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"ü§ñ Loading Llama-Primus-Reasoning Model\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"trendmicro-ailab/Llama-Primus-Reasoning\"\n",
    "DEVICE = \"auto\"\n",
    "TORCH_DTYPE = torch.float16\n",
    "\n",
    "# Quantization configuration (for memory efficiency)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "print(f\"üìã Model Configuration:\")\n",
    "print(f\"  - Model: {MODEL_NAME}\")\n",
    "print(f\"  - Device: {DEVICE}\")\n",
    "print(f\"  - Data Type: {TORCH_DTYPE}\")\n",
    "print(f\"  - 4-bit Quantization: Enabled\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    print(\"\\nüì• Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        use_fast=True\n",
    "    )\n",
    "    \n",
    "    # Set padding token if not present\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"‚úÖ Tokenizer loaded successfully\")\n",
    "    print(f\"   - Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"   - Pad token: {tokenizer.pad_token}\")\n",
    "\n",
    "    # Clear cache before loading model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load model\n",
    "    print(\"\\nüß† Loading model (this may take 5-10 minutes)...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=DEVICE,\n",
    "        torch_dtype=TORCH_DTYPE,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   - Model parameters: {model.num_parameters():,}\")\n",
    "    print(f\"   - Device mapping: {model.hf_device_map}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"   - GPU Memory: {memory_used:.1f}/{memory_total:.1f} GB ({memory_used/memory_total*100:.1f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "    print(\"\\nüí° Possible solutions:\")\n",
    "    print(\"1. Check your Hugging Face authentication\")\n",
    "    print(\"2. Ensure you have sufficient GPU memory\")\n",
    "    print(\"3. Try restarting the runtime\")\n",
    "    model, tokenizer = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_test"
   },
   "source": [
    "## 7. Test Model Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_model"
   },
   "outputs": [],
   "source": [
    "# Test model functionality\n",
    "if model is not None and tokenizer is not None:\n",
    "    print(\"üß™ Testing Model Functionality\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Test prompts\n",
    "    test_prompts = [\n",
    "        \"What is cybersecurity?\",\n",
    "        \"Explain the concept of risk assessment in simple terms.\",\n",
    "        \"What are the basic principles of network security?\"\n",
    "    ]\n",
    "\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\nüìù Test {i}: {prompt}\")\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=True\n",
    "            ).to(model.device)\n",
    "\n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the generated part\n",
    "            if prompt in response:\n",
    "                generated_text = response[len(prompt):].strip()\n",
    "            else:\n",
    "                generated_text = response.strip()\n",
    "\n",
    "            print(f\"üí¨ Response: {generated_text[:200]}{'...' if len(generated_text) > 200 else ''}\")\n",
    "            \n",
    "            # Clean up memory\n",
    "            del inputs, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "    print(\"\\n‚úÖ Model functionality test completed!\")\nelse:\n",
    "    print(\"‚ùå Model not loaded. Please complete the model loading step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### ‚úÖ Completed in this notebook:\n",
    "1. ‚úÖ GPU environment verification\n",
    "2. ‚úÖ Repository cloned from GitHub\n",
    "3. ‚úÖ Dependencies installed\n",
    "4. ‚úÖ Environment configured\n",
    "5. ‚úÖ Llama-Primus-Reasoning model loaded\n",
    "6. ‚úÖ Model functionality tested\n",
    "\n",
    "### üìã Next notebooks to run:\n",
    "1. **`02_benchmark_testing.ipynb`** - Run Phase 2-3 benchmarks\n",
    "2. **`03_results_analysis.ipynb`** - Analyze benchmark results\n",
    "\n",
    "### üîß Setup Required:\n",
    "- Set up API keys in Colab secrets (OpenAI, Anthropic, DeepSeek)\n",
    "- Configure Google Drive access for result storage\n",
    "- Ensure Hugging Face authentication is working\n",
    "\n",
    "### üìä Ready for Phase 2:\n",
    "The environment is now ready for Phase 2 baseline screening with 40 test cases (B1-B6 benchmarks)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}