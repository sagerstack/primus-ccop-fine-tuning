# Fine-Tuning Language Model on CCoP 2.0 Standards

## Project Objective
Read the project paper [Primus-Fine-Tuning-CCOP2-SG-v2.0-SagarPratapSingh-1010736.md](report/term1-mid/Primus-Fine-Tuning-CCOP2-SG-v2.0-SagarPratapSingh-1010736.md)

## Project References

### Regulatory & Standards References
- [Cyber Security Agency of Singapore - Codes of Practice](https://www.csa.gov.sg/legislation/codes-of-practice) - Official CCoP 2.0 standards and documentation
- [CCoP Second Edition Revision One PDF](https://isomer-user-content.by.gov.sg/36/2df750a7-a3bc-4d77-a492-d64f0ff4db5a/CCoP---SecondEdition_Revision-One.pdf) - Complete CCoP 2.0 regulatory document
- Local: `ccop-official/CCoP---Second-Edition_Revision-One.pdf` - Official CCoP 2.0 document (local copy)

### Supplementary CCoP 2.0 Documents (Local Repository)
- `ccop-official/supplementary/Guidelines_for_Auditing_Critical_Information_Infrastructure.pdf` - Auditing guidelines for CII
- `ccop-official/supplementary/Guide-to-Cyber-Threat-Modelling.pdf` - Threat modelling framework
- `ccop-official/supplementary/Guide-to-Conducting-Cybersecurity-Risk-Assessment-for-CII.pdf` - Risk assessment methodology
- `ccop-official/supplementary/Security_By_Design_Framework.pdf` - Security by design principles
- `ccop-official/references/Ensign's_Cybersecurity_Guide_on_CCoP_2_0_for_CII_Sep_2022.pdf` - Implementation guide for CCoP 2.0

### Model & Framework References
- [Llama-Primus-Reasoning on Hugging Face](https://huggingface.co/trendmicro-ailab/Llama-Primus-Reasoning) - Base cybersecurity-specialized reasoning model
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) - Parameter-efficient fine-tuning methodology

### Research & Analysis References
- [Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training](https://arxiv.org/html/2502.11191v1) - Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training
- [Learning and Forgetting Unsafe Examples in Large Language Models](https://arxiv.org/abs/2312.12736) - Research on catastrophic forgetting in sequential fine-tuning
- [Chained Tuning Leads to Biased Forgetting](https://arxiv.org/abs/2412.16469) - Research on sequential training challenges
- [The Ultimate Guide to Fine-Tuning LLMs](https://arxiv.org/pdf/2408.13296) - Comprehensive fine-tuning methodology reference
- [LalaEval: Human Evaluation Framework for Domain-Specific LLMs](https://arxiv.org/abs/2408.13338) - Evaluation framework for specialized language models
- [CyberLLMInstruct Dataset Analysis](https://arxiv.org/html/2503.09334v2) - Cybersecurity fine-tuning dataset research

### Industry & Implementation References
- [CyberSierra CCoP 2.0 Analysis](https://cybersierra.co/blog/ccop-2-regulations/) - Industry commentary on CCoP 2.0 implementation challenges
- [Thomson Reuters AI Compliance Research](https://www.thomsonreuters.com/en-us/posts/technology/expert-ai-automating-compliance-tasks) - Enterprise AI accuracy requirements
- [US GSA CUI Protection Guide](https://www.gsa.gov/system/files/Protecting-CUI-Nonfederal-Systems-%5BCIO-IT-Security-21-112-Initial-Release%5D-05-27-2022.pdf) - Reference for 85% accuracy threshold

### Project Planning Documents (Local Repository)
- `docs/phase1/phase1-user-story.md` - Phase 1 baseline evaluation infrastructure user story and functional requirements
- `docs/phase1/domain-specific-compliance-models-analysis.md` - Analysis of CyberLLM, SecLLM, RegBERT evaluation methodologies
- `docs/phase1/related-works-literature-review.md` - Literature review of LLM-based compliance checking models
- `ccop-official/RESPONSE-TO-FEEDBACK.pdf` - Official clarifications and responses on CCoP 2.0 requirements

### Repository Structure Guidance
- 1. SKIP files under research/archived* folder. They are not relevant to the context 
